{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hamblog\n",
    "By Kyle Hambrook"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<a href=\"https://khambroo.github.io/hamblog/\">HOME</a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Inference for Correlation: Asymptotic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Statistical Inference Problems for Correlation $\\rho$\n",
    "\n",
    "Let $X$ and $Y$ be random variables. We consider two statistical inference problems for the correlation $\\rho = \\rho(X,Y)$. \n",
    "\n",
    "Problem 1: Test the null hypothesis $H_0$: $\\rho = 0$ by calculating a $p$-value.\n",
    "\n",
    "Problem 2: Determine a confidence interval that contains $\\rho$ with prescribed probability.\n",
    "\n",
    "In this post, we consider asymptotic methods for these problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Asymptotic Distribution of Sample Correlation $r_n$\n",
    "\n",
    "Let $X_1,\\ldots,X_n$ and $Y_1,\\ldots,Y_n$ be random samples of $X$ and $Y$. Let $r_n = r_n(X_1,\\ldots,X_n,Y_1,\\ldots,Y_n)$ be the sample correlation.\n",
    "\n",
    "<strong> Theorem 1. </strong>\n",
    "Assume $E(X^4) < \\infty$, $E(Y^4) < \\infty$. Let $(X_i)_{i=1}^{\\infty} \\sim_{iid} X$ and $(Y_i)_{i=1}^{\\infty} \\sim_{iid} Y$ be infinite random samples. Then\n",
    "$$\n",
    "\\sqrt{n}(r_n-\\rho) \\rightarrow_d N_1(0,\\phi^2),\n",
    "$$\n",
    "where $\\phi$ is the positive number defined by \n",
    "$$\n",
    "\\phi^2 = \\frac{1}{4}\\rho^2( E(X_{\\ast}^4) + E(Y_{\\ast}^4) + 2E(X_{\\ast}^2 Y_{\\ast}^2) ) \n",
    "- \\rho(  E(X_{\\ast}^3 Y_{\\ast}) +  E(X_{\\ast} Y_{\\ast}^3)  )   +   E(X_{\\ast}^2 Y_{\\ast}^2),\n",
    "$$\n",
    "with $X_{\\ast} = (X - E (X))/\\sqrt{\\text{Var} (X)}$ and $Y_{\\ast} = (Y - E (Y))/\\sqrt{\\text{Var} (Y)}$.\n",
    "\n",
    "<strong> Remark. </strong> In general, the variance of the limiting distribution depends on higher moments of $(X,Y)$. However, if $\\rho = 0$, the variance of the limiting distribution is \n",
    "$$\n",
    "\\phi^2 = E(X_{\\ast}^2 Y_{\\ast}^2) \n",
    "= E(X_{\\ast}^2) E(Y_{\\ast}^2) \n",
    "= \\frac{\\text{Var}(X)}{\\text{Var}(X)} \\cdot \\frac{\\text{Var}(Y)}{\\text{Var}(Y)}\n",
    "=1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Hypothesis Test Based on Sample Correlation $r_n$\n",
    "\n",
    "The null hypothesis is $H_0$ : $\\rho = 0$.\n",
    "\n",
    "Let $r_n'$ be an observed value (or realization) of $r_n$. Precisely, $r_n' = r_n(X_1(\\omega'),\\ldots, X_n(\\omega'),Y_1(\\omega'),\\ldots,Y_n(\\omega'))$ for some fixed outcome $\\omega'$ in the sample space.\n",
    "\n",
    "Informally, the $p$-value is the probability of $r_n$ being at least as extreme as the observed value $r_n'$ assuming $H_0$ is true. What \"extreme\" means depends on the distribution of $r_n$. We give a precise defintion of the $p$-value below, but first we discuss its meaning.\n",
    "\n",
    "A small $p$-value is often interpreted as evidence against the null hypothesis $H_0$. However, let us emphasize what the $p$-value is not. It is __not__ the probability that $H_0$ is true. It is __not__ the probability of $H_0$ given the observed data. <!--It is __not__ the probability of $r_n$ being equal to the observed value assuming that $H_0$ is true.-->\n",
    "\n",
    "The following figure (borrowed from wikipedia) helps illustrate the concept of $p$-value.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/3a/P-value_in_statistical_significance_testing.svg\" alt=\"alt text\" style=\"width: 400px;\" title=\"Logo Title Text 1\">\n",
    "\n",
    "To compute the $p$-value, we need the distribution of $r_n$. We use the asymptotic approximation suggested by Theorem 1: $r_n \\sim N(0,n)$ given $H_0$.\n",
    "\n",
    "If $r_n' > 0$, the one-sided $p$-value is\n",
    "$$\n",
    "P(r_n \\geq r_n' \\; | \\;  H_0) = P(\\sqrt{n} r_n \\geq \\sqrt{n} r_n' \\; | \\;  H_0).\n",
    "$$\n",
    "It is the area of the right-tail region under the standard normal pdf to the right of $r_n'$ (assuming the asymptotic approximation for the distribution of $r_n$). \n",
    "\n",
    "If $r_n' < 0$, the one-sided $p$-value is\n",
    "$$\n",
    "P(r_n \\leq r_n' \\; | \\;  H_0) = P(\\sqrt{n} r_n \\leq \\sqrt{n} r_n' \\; | \\;  H_0).\n",
    "$$\n",
    "It is the area of the left-tail region under the standard normal pdf to the left of $r_n'$ (assuming the asymptotic approximation for the distribution of $r_n$). \n",
    "\n",
    "The two-sided $p$-value is\n",
    "$$\n",
    "P(r_n \\leq -|r_n'| or r_n \\geq |r_n'|  \\; | \\;  H_0) = 1 - P( -|r_n'| \\leq r_n \\leq |r_n'| \\; | \\;  H_0).\n",
    "$$ \n",
    "It is the area of the left-tail region under the standard normal pdf to the left of $-|r_n'|$ plus the area of the right-tail region to the right of $|r_n'|$ (assuming the asymptotic approximation for the distribution of $r_n$). \n",
    "\n",
    "For simplicity, let's focus on the one-sided $p$-value when $r_n' < 0$. For brevity, denote the $p$-value by\n",
    "$$\n",
    "p_n = P(r_n \\leq r_n' \\; | \\;  H_0).\n",
    "$$\n",
    "\n",
    "By Theorem 1, we know\n",
    "$$\n",
    "\\lim_{n \\to \\infty} p_n = \\Phi(\\sqrt{n} r_n').\n",
    "$$\n",
    "where $\\Phi$ is the standard normal cdf. The asymptotic approximation is\n",
    "$$\n",
    "p_n = \\Phi(\\sqrt{n} r_n').\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Python Example 1\n",
    "\n",
    "In Python Example 1, we will test the asymptotic approximation by repeated sampling. Here are the steps:\n",
    "\n",
    "1. Choose a distribution for $(X,Y)$ with $\\rho(X,Y) = 0$ ($H_0$ true).\n",
    "2. Choose a value of $n$.\n",
    "3. Draw a size-$n$ sample of $(X,Y)$ and compute $r_n$. This is $r_n'$ \n",
    "4. Compute $r_n$ for many more size-$n$ samples of $(X,Y)$. Say $N$ samples. \n",
    "5. Record $K$ = number of samples where $r_n \\leq r_n'$. \n",
    "6. Treat $K/N$ as $p_n = P(r_n \\leq r_n' \\; | \\;  H_0)$. By the law of large numbers, $K/N$ converges to $p_n$ as $N$ gets large.\n",
    "7. Compute the error $|p_n - \\Phi(\\sqrt{n} r_n')|$.\n",
    "8. Repeat steps 3 to 7 many times. And compute the average error.\n",
    "9. Repeat steps 2 to 8 many times. Plot the average error vs n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI. Python Example 2\n",
    "\n",
    "In Python Example 2, we will test the asymptotic approximation another way. To explain the test, first we need some theory.\n",
    "\n",
    "Remember that a random variable is just a (measurable) function from the sample space to the real numbers. The $p$-value $p_n$ can be viewed as a random variable.  To each outcome $\\omega'$ in the sample space, assign the real number\n",
    "$$\n",
    "p_n(\\omega') = P(r_n \\leq r_n' \\; | \\;  H_0)\n",
    "$$\n",
    "where $r_n' = r_n(X_1(\\omega'),\\ldots, X_n(\\omega'),Y_1(\\omega'),\\ldots,Y_n(\\omega'))$.\n",
    "\n",
    "<strong>Theorem 2. </strong>\n",
    "The $p$-value $p_n$ is uniformly distributed on $[0,1]$: $P(p_n \\leq \\alpha) = \\alpha$ for all $\\alpha \\in [0,1]$.\n",
    "\n",
    "Fix $\\alpha \\in (0,1)$. If $p_n \\leq \\alpha$ and the null hypothesis $H_0$ is true, we say we have a Type 1 error at significance level $\\alpha$. Remember that, if $\\alpha$ is small, the statement $p_n \\leq \\alpha$ is viewed as evidence against the null hypothesis. So a Type 1 error is \"$p$-value says $H_0$ false but $H_0$ is true\"\n",
    "\n",
    "The probability of a Type 1 error at signifiance level $\\alpha$ is $P(p_n \\leq \\alpha) = \\alpha$. \n",
    "\n",
    "We want to compare $P(p_n \\leq \\alpha) = \\alpha$ to $P(\\Phi(\\sqrt{n} r_n') \\leq \\alpha)$\n",
    "\n",
    "Here are the steps for test in Python Example 2.\n",
    "\n",
    "1. Choose a distribution for $(X,Y)$ with $\\rho(X,Y) = 0$ ($H_0$ true).\n",
    "2. Choose $\\alpha \\in (0,1)$. \n",
    "3. Choose a value of $n$.\n",
    "4. Sample from $(X,Y)$ and compute \\Phi( \\sqrt{n} r_n')\n",
    "5. Repeat Step 4 $N$ times. Record $K$ = number of times $\\Phi( \\sqrt{n} r_n') \\leq \\alpha$.\n",
    "6. Treat $K / N$ as $P(\\Phi( \\sqrt{n} r_n') \\leq \\alpha)$. By the law of large numbers, $K/N$ converges to $P(\\Phi( \\sqrt{n} r_n') \\leq \\alpha)$ as $N$ gets large.\n",
    "7. Compute the error $|\\alpha - P(\\Phi(\\sqrt{n} r_n') \\leq \\alpha)|$\n",
    "8. Repeat 3-7 for many values of $n$. Plot the error vs n\n",
    "9. Repeat 2 to 8 for many values of $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII. Confidence Interval Based on Sample Correlation\n",
    "\n",
    "Confidence interval. Issue because of $\\phi$. Estimate it.\n",
    "\n",
    "Let $\\theta \\in \\mathbb{R}$ be a parameter for the distribution of $X$. \n",
    "<!--Suppose $T$ is an estimator of $\\theta$.-->\n",
    "Let $I(T)$ be an interval whose endpoints are functions of $T$. \n",
    "\n",
    "$I(T)$ is called a $95\\%$ confidence interval for $\\theta$ based on $T$ if $P(\\theta \\in I(T)) \\geq 0.95$. Let $\\alpha \\in (0,1)$. $I(T)$ is called a $(1-\\alpha)\\%$ confidence interval for $\\theta$ based on $T$ if $P(\\theta \\in I(T)) \\geq 1-\\alpha$. \n",
    "\n",
    "Obviously, $\\mathbb{R}$ is a $(1-\\alpha)100\\%$ confidence interval for every $\\alpha \\in (0,1)$. We usually want the smallest $(1-\\alpha)100\\%$ confidence interval possible. We also often want the confidence interval to be centered at $T$.\n",
    "\n",
    "__Remark.__ The hypothesis test for $\\rho = \\rho_0$, where $\\rho_0$ is a fixed non-zero number, has the same issue.\n",
    "\n",
    "Another solution is in the next post. Fisher transformation.\n",
    "\n",
    "Unfortunately, the $\\phi$ on the right-hand side means we can't compute this approximation. To overcome this, we will estimate \n",
    "$$\n",
    "\\phi^2 = E(X_{\\ast}^2 Y_{\\ast}^2) = E\n",
    "$$\n",
    "by \n",
    "$$\n",
    "\\widehat{\\phi^2} = \\frac{1}{n} \\sum_{i=1}^{n} \n",
    "$$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a href=\"https://khambroo.github.io/hamblog/\">HOME</a> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
